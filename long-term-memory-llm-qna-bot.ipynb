{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a99131",
   "metadata": {},
   "source": [
    "# Python LLM Q&A Bot with Long-Term Memory\n",
    "\n",
    "If you've been working with LLMs like ChatGPT for a while, you'll know that ChatGPT's knowledge cut-off is at the year 2021. This means _it doesn't know events or information beyond that point._\n",
    "\n",
    "For example, if you ask it about major events in the year 2023, it's not going to be able to answer.\n",
    "\n",
    "So yeah, for anything beyond 2021, you will have to teach it yourself. One way is to give it some context along with your questions. But that only works for something not too long and it mostly works when you want a summary of something because, logically speaking, if you already have the answer, why ask a bot again.\n",
    "\n",
    "And for longer or larger context, like books or private documents, it's unrealistic to do it.\n",
    "\n",
    "So instead, what if we store our large text data in some sort of database and let the bot query the data and answer our question based on that?\n",
    "\n",
    "Yes, it is possible to do that. We're going to utilize what we call a **vector database**, a type of database that stores data in a high-dimensional vector form. That data can be many things: text, sound, image, etc.\n",
    "\n",
    "Vector database is different from traditional database in that, with traditional ones like SQL, your queries have to be exact in order to get the correct data. With vector database, there's something called \"similarity search\", where you get the results based on the similarity of your queries and the information stored in the database.\n",
    "\n",
    "And because of that, it's quite good for working with something like recommendation system, asking questions, text search, etc.\n",
    "\n",
    "Enough about that. Let's get to building stuff.\n",
    "\n",
    "## What We're Going to Build\n",
    "\n",
    "This mini-project we're going to be working on is essentially a GPT-based chatbot that can answer stuff that doesn't exist in GPT's knowledge such as things beyond the year 2021, private documents that aren't available publically, or product-specific information.\n",
    "\n",
    "For example, you can feed it an internal rules or code of conduct for your company so your employees can ask about them easily without having to read through the entire document just to get an answer.\n",
    "\n",
    "### Tools and Frameworks\n",
    "\n",
    "These are the tools and frameworks we're going to be using:\n",
    "- **Language:** Python\n",
    "- [**ChromaDB**](https://www.trychroma.com/) - An open-source vector database you can host on your machine.\n",
    "    - You can also use other vector database like [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), and etc. But for simplicity's sake, I will use Chroma's transient storage to store the data.\n",
    "- [**LangChain**](https://python.langchain.com/docs/get_started/introduction.html) - A framework that makes it much easier to work with LLMs and vector database. It basically acts as a bridge between language models and vector database. It also provides many functionalities that help prompt generation and query processing.\n",
    "- [**OpenAI**](https://openai.com/) - This will be our LLM bot that convert the data queried from the database into proper sentences.\n",
    "\n",
    "**Python Packages**\n",
    "\n",
    "These are the Python packages we're going to be using:\n",
    "- `python-dotenv`\n",
    "- `langchain`\n",
    "- `tiktoken`\n",
    "- `wikipedia`\n",
    "- `chromadb`\n",
    "\n",
    "### AN IMPORTANT NOTE\n",
    "\n",
    "LangChain is still very early in the development and is constantly being updated with more features. The code you see in this project will very likely break in the future. If you're stuck, check out the official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8ad4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q openai langchain python-dotenv tiktoken chromadb wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1448a",
   "metadata": {},
   "source": [
    "## Let's get started\n",
    "\n",
    "Below are all the packages we will need. You might be confused as to which does what. Don't worry. **I will also be importing them in the cells that require them** so you know when to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72942956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e3915",
   "metadata": {},
   "source": [
    "### Prepare The Environment\n",
    "\n",
    "We're going to be utilizing an environment variable named `OPENAI_API_KEY`. This is entirely optional and you can pass the key manually in the functions that need it. I'll be using it to save some time.\n",
    "\n",
    "An example of `.env` file:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21421b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "# This will set the API key for OpenAI so we don't have to manually enter it later\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21b779",
   "metadata": {},
   "source": [
    "`load_dotenv()` loads the environment varibles from a specified file. In this case, it loads from the `.env` file found in the root folder with `find_dotenv()`.\n",
    "\n",
    "### Prepare And Store Text Data As Vectors\n",
    "\n",
    "Before we can ask a bot to answer our questions, we need to have the information that the bot will be using first.\n",
    "\n",
    "#### Load Text Data\n",
    "\n",
    "Let's get some text content. LangChain supports tons of ways to load text ranging from `.docx` or `.pdf` to scraping websites like Wikipedia or Hacker News.\n",
    "\n",
    "Check out LangChain's [document loaders](https://python.langchain.com/docs/integrations/document_loaders/) for a complete list of loaders it supports.\n",
    "\n",
    "In this project, We will be loading the Apple Inc. Wikipedia page. This will be the content that we will store in a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e866549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "# Let's load a Wikipedia page about YouTuber\n",
    "loader = WikipediaLoader(query='Apple Inc.', lang='en', load_max_docs=30)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0c30c",
   "metadata": {},
   "source": [
    "`WikipediaLoader()` creates a loader instance. The loader instance has the `load()` function that loads Wikipedia documents, clean them, and return the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "785b1be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c87dd",
   "metadata": {},
   "source": [
    "#### Split Text Into Chunks\n",
    "\n",
    "To store the text data as vectors, we need to split them into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648e9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cd7b2",
   "metadata": {},
   "source": [
    "`RecuriveCharacterTextSplitter` splits text into smaller chunks. The `chunk_size` parameter is the length of each chunk. By default, the length of each chunk is determined by the number of characters (using `len()`), but you can change it to other functions by passing a function to the `length_function` parameter. In this project, I'm going to use the default `len` function. `chunk_overlap` is how much should one chunk's text overlap with the next.\n",
    "\n",
    "#### Vector Embedding and Storing Data\n",
    "\n",
    "Vector embedding is a way to turn data into vectors, numerical representation of pieces of information, that can be stored in vector databases. Vectors are used to compare pieces of data. The closer their _directions_ to each other, the more similar the pieces are.\n",
    "\n",
    "To embed vectors, we'll have to rely on embedding models. In this project, we will use the embedding model \"Ada v2\" by OpenAI.\n",
    "\n",
    "Vector embedding costs money so let's calculate the cost. Check out OpenAI's [pricing](https://openai.com/pricing) page for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20869d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 8472\n",
      "Total cost (USD): 0.000847\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of tokens used for the documents\n",
    "import tiktoken\n",
    "\n",
    "# Currently, there are only one embedding model: 'text-embedding-ada-002'\n",
    "enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "token_count = sum([len(enc.encode(page.page_content)) for page in chunks])\n",
    "\n",
    "# Embedding cost per 1000 tokens as of the time of this writing\n",
    "embedding_cost = 0.0001\n",
    "total_cost = (token_count / 1000) * embedding_cost\n",
    "\n",
    "print(f'Token count: {token_count}')\n",
    "print(f'Total cost (USD): {total_cost:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db9ce2",
   "metadata": {},
   "source": [
    "Let's continue on with text embedding. As mentioned earlier, we will be using OpenAI's text embedding model.\n",
    "\n",
    "LangChain supports many vector databases such as Chroma, Pinecone, Milvus, and Weaviate. For simplicity's sake, we will store the text data along with its vector embeddings in a transient Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abdc47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create an OpenAI Embedding model instance\n",
    "# using the API key from the environment variable OPENAI_API_KEY\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# or do this if you want to manually enter your API key\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=\"my-api-key\")\n",
    "\n",
    "# Store the chunks and their vector embeddings with Chroma\n",
    "# and return a vectorstore object\n",
    "# This is a transient database, meaning it's not persistent\n",
    "# and will be gone with the session is closed.\n",
    "# To make the data persistent, pass in `persist_directory=\"./chroma_db\"`\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a2f77",
   "metadata": {},
   "source": [
    "`OpenAIEmbeddings` creates an embedding model instance, and `Chroma.from_documents` uses that model to \"embed\" the text data with vectors and store them inside a database and then returns you a `vectorstore` object which can be used to query data.\n",
    "\n",
    "### Setup A Q&A Bot\n",
    "\n",
    "Next, let's setup a GPT bot that we will be using to ask questions about the data we just stored. We're going to use GPT-3.5 Turbo as our AI assistent to answer our questions. You can also use GPT-4 if your OpenAI account is eligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142ff89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "\n",
    "# Create a OpenAI LLM instance\n",
    "# You can also try the GPT-4 model if your account is eligible\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.7)\n",
    "\n",
    "# Turn the Chroma vectorstore into a retriever to be used in the chain\n",
    "# You can increase the 'k' value to make it return more results for better context\n",
    "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 10})\n",
    "\n",
    "# Create a Q&A chain instance\n",
    "# Use RetrievalQA instead if you don't want the ability to have a chat history\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17dca3b",
   "metadata": {},
   "source": [
    "`ChatOpenAI` creates an OpenAI LLM instance. This is the chat model that will basically look at the queried text data and answer our questions.\n",
    "\n",
    "`ConversationalRetrievalChain.from_llm` is a LangChain function that creates an Conversational Chain instance that handles fetching of data from a vector store, create a prompt for the LLM, and then return the answer. It can also retain \"memory\" and use previous questions as the context to answer other questions, too.\n",
    "\n",
    "### Ask Questions\n",
    "\n",
    "Now, we're ready to ask our bot questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27be7c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple was founded on April 1, 1976.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty list to store chat history\n",
    "chat_history = []\n",
    "\n",
    "# Ask the chat bot something\n",
    "query = \"When was Apple founded?\"\n",
    "\n",
    "# Get the result\n",
    "result = qa_chain({\"question\": query,\n",
    "                   \"chat_history\": chat_history\n",
    "                  })\n",
    "\n",
    "# Append the chat history\n",
    "# The format is (question, answer)\n",
    "chat_history.append((query, result['answer']))\n",
    "\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fa9ea",
   "metadata": {},
   "source": [
    "In this project, we'll be storing chat history inside a simple list. That format for each object in chat history is `(question, answer)`.\n",
    "\n",
    "We ask a question by calling the chain (in this case, `qa_chain`) and passing in a question along with the chat history list. The bot will return a result object. This is where we record the answer to the `chat_history` list.\n",
    "\n",
    "As you can see, the bot is able to answer our question. Now, what if we ask it to multiply the year number by 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "533a3179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The year Apple was founded is 1976. When you multiply 1976 by 2, the result is 3952.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask the chat bot something again\n",
    "query = \"Multiply the year by 2\"\n",
    "\n",
    "# Get the result\n",
    "result = qa_chain({\"question\": query,\n",
    "                   \"chat_history\": chat_history\n",
    "                  })\n",
    "\n",
    "# Append the chat history\n",
    "chat_history.append((query, result['answer']))\n",
    "\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9e427",
   "metadata": {},
   "source": [
    "As you can see, even if we kept the question vague by not specifying what year we were referring to, the bot successfully deduced that we meant the year Apple was founded because of the previous question.\n",
    "\n",
    "Here is the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18945d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When was Apple founded?', 'Apple was founded on April 1, 1976.'),\n",
       " ('Multiply the year by 2',\n",
       "  'The year Apple was founded is 1976. When you multiply 1976 by 2, the result is 3952.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642e883",
   "metadata": {},
   "source": [
    "And that's it! LangChain and vector database are very powerful. We can utilize this with many things like a chat bot for answering customers about your products or services, or a bot for answering questions regarding rules and legal documents. The possibilities are endless."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
